[1mdiff --git a/src/main.py b/src/main.py[m
[1mindex cb9eabb..cf931ae 100644[m
[1m--- a/src/main.py[m
[1m+++ b/src/main.py[m
[36m@@ -24,7 +24,7 @@[m [mdef get_data_frame():[m
 # test_message_tokenized = tokenizer.tokenize(test_message)[m
 # test_message_tokenized = ['Hey','GGggGG','feet','it','going','HTML','bads','bads','randoms','badly'][m
 def message_to_token_list(message):[m
[31m-    tokenizer = nltk.RegexpTokenizer(r"\w+") # A tokenizer that splits a string using a regular expression[m
[32m+[m[32m    tokenizer = nltk.RegexpTokenizer(r"\w+")  # A tokenizer that splits a string using a regular expression[m
     tokens = tokenizer.tokenize(message)[m
     lowercased_tokens = [t.lower() for t in tokens][m
     lemmatized_tokens = [WordNetLemmatizer().lemmatize(t) for t in lowercased_tokens][m
[36m@@ -95,7 +95,7 @@[m [mfor token in TOKEN_COUNTER:[m
 bag_of_words = list(bag_of_words)[m
 [m
 # create a map with the bag of words and give an index to each one[m
[31m-token_to_index_mapping = {t: i for t, i in zip(bag_of_words, range(len(bag_of_words)))}[m
[32m+[m[32mtoken_to_index_mapping = {t:i for t, i in zip(bag_of_words, range(len(bag_of_words)))}[m
 [m
 x_train, y_train = generate_results(train_df)[m
 x_test, y_test = generate_results(test_df)[m
